{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bff5cb4",
   "metadata": {},
   "source": [
    "# Forward ad backward passes\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ebb8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MNIST dataset\n",
    "import gzip\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "MNIST_URL = \"https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true\"\n",
    "\n",
    "data_path = Path(\"data/mnist\")\n",
    "data_path.mkdir(exist_ok=True)\n",
    "gz_path = data_path / \"mnist.pkl.gz\"\n",
    "\n",
    "mpl.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "if not gz_path.exists():\n",
    "    urlretrieve(MNIST_URL, gz_path)\n",
    "\n",
    "# File contains a tuple of tuples for the x and y, train and validation data\n",
    "# Images are 28x28\n",
    "with gzip.open(gz_path, \"rb\") as file:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(file, encoding=\"latin-1\")\n",
    "\n",
    "# Put into tensors\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n",
    "\n",
    "# We want our data as floats so we can use MSE\n",
    "x_train = x_train.float()\n",
    "y_train = y_train.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e31de",
   "metadata": {},
   "source": [
    "## Foundations version\n",
    " \n",
    "- consider a linear model straight line on a graph. Limiting, what if what we want to approximate is a curvy line.\n",
    "- summing multiple rectified lines/models together lets us create any shape we wanted if we had enough lines\n",
    "- works the same in N dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e625f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10.))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets define some constants that describe the data\n",
    "n_examples, n_pixels = x_train.shape\n",
    "possible_values = y_train.max() + 1\n",
    "\n",
    "# How many nodes/activations/line thingys\n",
    "n_hidden = 50\n",
    "\n",
    "n_examples, n_pixels, possible_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8dbab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# For a simple linear model\n",
    "# 50000x784 (examples x pixels)  @ 784x10 (pixels x weights for each possible value)\n",
    "# = 50000x10 (results for 50000 images)\n",
    "#\n",
    "# However we have 50 hidden layers so we want to to go into 784x50 and then 50000x10.\n",
    "\n",
    "# Weights and bias for hidden layer\n",
    "h_weights = torch.randn(n_pixels, n_hidden)  # 784x50\n",
    "h_bias = torch.zeros(n_hidden)\n",
    "\n",
    "# Output layer, for now we are just going to create 1 output rather than 10 to tell use the prediced number\n",
    "# This will be a bit daft when we calculate the loss (as numbers further away from each other aren't more wrong)\n",
    "# but makes some of the other calculations easier for now\n",
    "o_weights = torch.randn(n_hidden, 1)  # Take outputs from hidden and smush into 1\n",
    "o_bias = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf382955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to put x through a linear layer with specified bweights and bias\n",
    "def lin(x, weights, bias):\n",
    "    return x @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce300c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 50])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = lin(x_train, h_weights, h_bias)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f95689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will need to put them relu\n",
    "def relu(x):\n",
    "    return x.clamp_min(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470014cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  9.2682,  ...,  1.3954,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  4.4059,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  9.7413,  ...,  9.1958,  6.1656,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., 10.5230,  0.0000,  3.1906],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  8.7968,  ...,  3.0232,  0.0000, 13.5904]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = relu(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e39d8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can put this toghether in a baslic MLP\n",
    "def model(in_data):\n",
    "    h_out = lin(in_data, h_weights, h_bias)\n",
    "    h_out = relu(h_out)\n",
    "\n",
    "    o_out = lin(h_out, o_weights, o_bias)\n",
    "\n",
    "    return o_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4667cd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[23.2929],\n",
       "         [51.0174],\n",
       "         [94.4659],\n",
       "         ...,\n",
       "         [90.0241],\n",
       "         [88.1075],\n",
       "         [83.3951]]),\n",
       " torch.Size([50000, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Well at least it tried\n",
    "res = model(x_train)\n",
    "res, res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69bfe86",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4712cb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 50000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because of the shape of our data and broadcasting rules we cant just do this\n",
    "(res - y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7364ef24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As the broadcasting rules have broadcast 500000 into our 1\n",
    "# We need to turn out 10000 vector into a 10000x1 matrix or turn our matrix into a vector\n",
    "res[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2625183b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .squeeze can do it too, it removes all unit dimensions\n",
    "res.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b25d8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res.squeeze() - y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da26c2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6555.3013)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So here is our mse\n",
    "def mse(result, actual):\n",
    "    return (result.squeeze() - actual).pow(2).mean()\n",
    "\n",
    "\n",
    "mse(res, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ddcc6a",
   "metadata": {},
   "source": [
    "## Gradients and backward pass\n",
    "\n",
    "We have `loss = L(f(x, weights), y)`. If we get the derivative of the loss with respect to a weight (`dloss/wi`) we would know how to change the weight to reduce the loss. We can then update each weight and go again. Thats SGD.\n",
    "\n",
    "We have a whole bunch of weights and a whole bunch of inputs (pixels). And we also have multiple images to input. So we and up with a matrix of derivatives (the Jacobian) (a row for every input and a col for every output). We eventually want to end up with a single number for every input as our loss will need to be a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02e51b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x$"
      ],
      "text/plain": [
       "2*x"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sympy could calulate derivatives symbolically for use\n",
    "from sympy import symbols, diff\n",
    "\n",
    "x, y = symbols(\"x y\")\n",
    "diff(x**2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04e7b4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 6 x$"
      ],
      "text/plain": [
       "6*x"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(3 * x**2 + 9, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4fe0e",
   "metadata": {},
   "source": [
    "Why is the derivative  of 3*x**2+9 = 6x?\n",
    "\n",
    "The derivative of `a**b` with respect to `a` is `ba**b-1`, `da**b/da = ba**b-1`.\n",
    "\n",
    "So we could rewrite as:\n",
    "\n",
    "```\n",
    "y = 3u + 9\n",
    "\n",
    "where:\n",
    "u = x**2\n",
    "```\n",
    "\n",
    "The derivative of a sum is the sum of the derivatives. The derivative of a constant is 0, as changes dont effect it. So\n",
    "\n",
    "```\n",
    "dy/du = 3 + 0\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```\n",
    "dy/dx = dy/du * du/dx  <-- this is the chain rule\n",
    "\n",
    "dy/dx = 2x * 3 = 6x\n",
    "```\n",
    "\n",
    "We want to calculate the gradient of our MSE with respect to the input weights of the model. We cant do it symbolically so we will start at the end and use the chain rule to calculate the derivative. This is called backpropagation.\n",
    "\n",
    "We can implement this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be8a2761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, weights, bias):\n",
    "    # grad of the matrix multiplication with respect to input\n",
    "    inp.g = out.g @ weights.t()\n",
    "\n",
    "    weights.g = inp.T @ out.g\n",
    "    # ^This is the same as:\n",
    "    # weights.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "\n",
    "    bias.g = out.g.sum(0)\n",
    "\n",
    "\n",
    "def forward_and_backward(inp, actual):\n",
    "    # forward pass (run the model)\n",
    "    l1 = lin(inp, h_weights, h_bias)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, o_weights, o_bias)\n",
    "\n",
    "    # MSE in 2 steps\n",
    "    diff = out[:, 0] - actual\n",
    "    loss = diff.pow(2).mean()\n",
    "\n",
    "    # backward pass (calc the gradient), well store the gradients for each layer in g, working backwards\n",
    "    out.g = 2.0 * diff[:, None] / inp.shape[0]  # deal with pow and mean\n",
    "\n",
    "    # grad of l2, we need to know grad of out to do chain rule\n",
    "    lin_grad(l2, out, o_weights, o_bias)\n",
    "    l1.g = (l1 > 0).float() * l2.g\n",
    "    lin_grad(inp, l1, h_weights, h_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca715306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  15.1010],\n",
       "        [  39.7500],\n",
       "        [ 940.9993],\n",
       "        [ 771.4215],\n",
       "        [1008.7953],\n",
       "        [ 598.8926],\n",
       "        [ 724.3143],\n",
       "        [ 206.8452],\n",
       "        [1034.5780],\n",
       "        [  93.9648],\n",
       "        [ 140.6766],\n",
       "        [ 465.4284],\n",
       "        [  44.4407],\n",
       "        [  34.9705],\n",
       "        [ 225.5081],\n",
       "        [ 591.0358],\n",
       "        [1565.5575],\n",
       "        [ 808.0443],\n",
       "        [1232.3121],\n",
       "        [ 110.6135],\n",
       "        [ 380.2662],\n",
       "        [1460.4283],\n",
       "        [1486.6355],\n",
       "        [ 436.7134],\n",
       "        [  48.7040],\n",
       "        [1058.4683],\n",
       "        [ 182.1762],\n",
       "        [  93.7271],\n",
       "        [ 845.0152],\n",
       "        [ 337.5440],\n",
       "        [ 301.0381],\n",
       "        [  31.8464],\n",
       "        [ 767.2654],\n",
       "        [1053.3126],\n",
       "        [   4.8145],\n",
       "        [ 616.2936],\n",
       "        [  28.1131],\n",
       "        [  30.1059],\n",
       "        [1250.2356],\n",
       "        [ 953.7845],\n",
       "        [1195.8608],\n",
       "        [  91.5056],\n",
       "        [ 238.6953],\n",
       "        [ 517.3145],\n",
       "        [ 625.9207],\n",
       "        [ 906.5762],\n",
       "        [ 255.8927],\n",
       "        [ 461.5306],\n",
       "        [  37.7155],\n",
       "        [ 260.7934]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_and_backward(x_train, y_train)\n",
    "o_weights.g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d29c68",
   "metadata": {},
   "source": [
    "## Refactor the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c3cb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimplement our functions a callable classes, that calculate their own gradient\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.0)\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    # Calculate our inputs gradient, chain with output grad\n",
    "    def backward(self):\n",
    "        self.inp.g = (self.inp > 0).float() * self.out.g\n",
    "\n",
    "\n",
    "class Lin:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = self.inp @ self.weights + self.bias\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.weights.t()\n",
    "        self.weights.g = self.inp.t() @ self.out.g\n",
    "        self.bias.g = self.out.g.sum(0)\n",
    "\n",
    "\n",
    "class Mse:\n",
    "    def __call__(self, inp, targets):\n",
    "        self.inp = inp\n",
    "        self.targets = targets\n",
    "\n",
    "        self.out = (self.inp.squeeze() - self.targets).pow(2).mean()\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.inp.g = 2.0 * (self.inp.squeeze() - self.targets).unsqueeze(-1) / self.targets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b5aa3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar story for out model\n",
    "class Model:\n",
    "    def __init__(self, weights1, bias1, weights2, bias2):\n",
    "        self.layers = [Lin(weights1, bias1), Relu(), Lin(weights2, bias2)]\n",
    "        self.loss = Mse()\n",
    "\n",
    "    def __call__(self, inp, targets):\n",
    "        result = inp\n",
    "        for layer in self.layers:\n",
    "            result = layer(result)\n",
    "\n",
    "        result = self.loss(result, targets)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Run backwards through our layers and calc the grad\n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fea4d5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6555.3013),\n",
       " tensor([[  15.1010],\n",
       "         [  39.7500],\n",
       "         [ 940.9993],\n",
       "         [ 771.4215],\n",
       "         [1008.7953],\n",
       "         [ 598.8926],\n",
       "         [ 724.3143],\n",
       "         [ 206.8452],\n",
       "         [1034.5780],\n",
       "         [  93.9648],\n",
       "         [ 140.6766],\n",
       "         [ 465.4284],\n",
       "         [  44.4407],\n",
       "         [  34.9705],\n",
       "         [ 225.5081],\n",
       "         [ 591.0358],\n",
       "         [1565.5575],\n",
       "         [ 808.0443],\n",
       "         [1232.3121],\n",
       "         [ 110.6135],\n",
       "         [ 380.2662],\n",
       "         [1460.4283],\n",
       "         [1486.6355],\n",
       "         [ 436.7134],\n",
       "         [  48.7040],\n",
       "         [1058.4683],\n",
       "         [ 182.1762],\n",
       "         [  93.7271],\n",
       "         [ 845.0152],\n",
       "         [ 337.5440],\n",
       "         [ 301.0381],\n",
       "         [  31.8464],\n",
       "         [ 767.2654],\n",
       "         [1053.3126],\n",
       "         [   4.8145],\n",
       "         [ 616.2936],\n",
       "         [  28.1131],\n",
       "         [  30.1059],\n",
       "         [1250.2356],\n",
       "         [ 953.7845],\n",
       "         [1195.8608],\n",
       "         [  91.5056],\n",
       "         [ 238.6953],\n",
       "         [ 517.3145],\n",
       "         [ 625.9207],\n",
       "         [ 906.5762],\n",
       "         [ 255.8927],\n",
       "         [ 461.5306],\n",
       "         [  37.7155],\n",
       "         [ 260.7934]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can build a model and run it\n",
    "model = Model(h_weights, h_bias, o_weights, o_bias)\n",
    "loss = model(x_train, y_train)\n",
    "model.backward()\n",
    "\n",
    "loss, o_weights.g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d02b5",
   "metadata": {},
   "source": [
    "### Refactor with a Module\n",
    "\n",
    "We can refactor with a Module base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31d212f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self):\n",
    "        raise Exception(\"Not Implemented\")\n",
    "\n",
    "    def bwd(self):\n",
    "        raise exception(\"Not implemented\")\n",
    "\n",
    "    def backward(self):\n",
    "        self.bwd(self.out, *self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0c47d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now our class implementations look like this\n",
    "\n",
    "\n",
    "class Relu(Module):\n",
    "    def forward(self, inp):\n",
    "        return inp.clamp_min(0.0)\n",
    "\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = (inp > 0).float() * out.g\n",
    "\n",
    "\n",
    "class Lin(Module):\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return inp @ self.weights + self.bias\n",
    "\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.weights.t()\n",
    "        self.weights.g = inp.t() @ out.g\n",
    "        self.bias.g = out.g.sum(0)\n",
    "\n",
    "\n",
    "class Mse(Module):\n",
    "    def forward(self, inp, targets):\n",
    "        return (inp.squeeze() - targets).pow(2).mean()\n",
    "\n",
    "    def bwd(self, out, inp, targets):\n",
    "        inp.g = 2.0 * (inp.squeeze() - targets).unsqueeze(-1) / targets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "53f8a60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6555.3013),\n",
       " tensor([[  15.1010],\n",
       "         [  39.7500],\n",
       "         [ 940.9993],\n",
       "         [ 771.4215],\n",
       "         [1008.7953],\n",
       "         [ 598.8926],\n",
       "         [ 724.3143],\n",
       "         [ 206.8452],\n",
       "         [1034.5780],\n",
       "         [  93.9648],\n",
       "         [ 140.6766],\n",
       "         [ 465.4284],\n",
       "         [  44.4407],\n",
       "         [  34.9705],\n",
       "         [ 225.5081],\n",
       "         [ 591.0358],\n",
       "         [1565.5575],\n",
       "         [ 808.0443],\n",
       "         [1232.3121],\n",
       "         [ 110.6135],\n",
       "         [ 380.2662],\n",
       "         [1460.4283],\n",
       "         [1486.6355],\n",
       "         [ 436.7134],\n",
       "         [  48.7040],\n",
       "         [1058.4683],\n",
       "         [ 182.1762],\n",
       "         [  93.7271],\n",
       "         [ 845.0152],\n",
       "         [ 337.5440],\n",
       "         [ 301.0381],\n",
       "         [  31.8464],\n",
       "         [ 767.2654],\n",
       "         [1053.3126],\n",
       "         [   4.8145],\n",
       "         [ 616.2936],\n",
       "         [  28.1131],\n",
       "         [  30.1059],\n",
       "         [1250.2356],\n",
       "         [ 953.7845],\n",
       "         [1195.8608],\n",
       "         [  91.5056],\n",
       "         [ 238.6953],\n",
       "         [ 517.3145],\n",
       "         [ 625.9207],\n",
       "         [ 906.5762],\n",
       "         [ 255.8927],\n",
       "         [ 461.5306],\n",
       "         [  37.7155],\n",
       "         [ 260.7934]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Everything still works and with less code\n",
    "model = Model(h_weights, h_bias, o_weights, o_bias)\n",
    "loss = model(x_train, y_train)\n",
    "model.backward()\n",
    "\n",
    "loss, o_weights.g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc2d181",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "Obviously pytorch has done all of this for us (`nn.Module`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f33d4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# We would implement out Lin class as this\n",
    "# Note we dont have to implment that backward pass at all, just ask pytorch to with requires_grad\n",
    "\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.weights = torch.randn(n_in, n_out).requires_grad_()\n",
    "        self.bias = torch.zeros(n_out).requires_grad_()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return inp @ self.weights + self.bias\n",
    "\n",
    "\n",
    "## We can reimplement our model using our Linear and borrwoing everything else from pytorch\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in, n_hidden), nn.ReLU(), Linear(n_hidden, n_out)]\n",
    "\n",
    "    def __call__(self, inp, targets):\n",
    "        res = inp\n",
    "        for layer in self.layers:\n",
    "            res = layer(res)\n",
    "\n",
    "        res = F.mse_loss(res, targets[:, None])\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "66f887a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1223.1824, grad_fn=<MseLossBackward0>),\n",
       " tensor([[ 4.3840e+01],\n",
       "         [ 1.1604e+02],\n",
       "         [-5.3228e+01],\n",
       "         [ 1.2646e+02],\n",
       "         [ 7.8353e+01],\n",
       "         [ 7.8051e+01],\n",
       "         [ 1.5563e+01],\n",
       "         [ 3.5694e+01],\n",
       "         [ 1.4275e+02],\n",
       "         [ 1.6304e+01],\n",
       "         [ 1.4957e+02],\n",
       "         [ 3.2792e+01],\n",
       "         [ 7.8994e+01],\n",
       "         [ 4.8192e+01],\n",
       "         [ 1.3647e+02],\n",
       "         [ 1.4502e+02],\n",
       "         [ 9.2334e+01],\n",
       "         [ 7.9152e+01],\n",
       "         [ 1.1404e+01],\n",
       "         [ 4.3579e+01],\n",
       "         [ 1.1250e+02],\n",
       "         [ 6.5240e+00],\n",
       "         [ 1.8020e+02],\n",
       "         [ 1.7331e+02],\n",
       "         [ 1.1430e+02],\n",
       "         [ 3.1277e+02],\n",
       "         [ 5.6202e+01],\n",
       "         [ 3.5445e+02],\n",
       "         [ 1.8891e+02],\n",
       "         [-1.8835e+01],\n",
       "         [ 1.4538e+02],\n",
       "         [ 1.4102e+01],\n",
       "         [ 2.5614e+01],\n",
       "         [ 1.4344e-01],\n",
       "         [ 4.2751e+02],\n",
       "         [ 6.4488e+01],\n",
       "         [ 3.9571e+01],\n",
       "         [ 8.8182e+01],\n",
       "         [ 4.8206e+02],\n",
       "         [-4.3477e+00],\n",
       "         [ 3.5290e+01],\n",
       "         [ 2.3634e+02],\n",
       "         [ 2.7755e+02],\n",
       "         [ 2.9625e+01],\n",
       "         [ 1.0536e+02],\n",
       "         [ 7.7752e+01],\n",
       "         [ 1.6198e+02],\n",
       "         [ 3.2209e+01],\n",
       "         [ 2.2112e+02],\n",
       "         [ 2.8573e+02]]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Everything still works and with even less code\n",
    "model = Model(n_pixels, n_hidden, 1)\n",
    "loss = model(x_train, y_train)\n",
    "loss.backward()\n",
    "\n",
    "loss, model.layers[2].weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3b942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
