{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8632f8f",
   "metadata": {},
   "source": [
    "# Minibatch training\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a851f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MNIST dataset\n",
    "import gzip\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "MNIST_URL = \"https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true\"\n",
    "\n",
    "data_path = Path(\"data/mnist\")\n",
    "data_path.mkdir(exist_ok=True)\n",
    "gz_path = data_path / \"mnist.pkl.gz\"\n",
    "\n",
    "mpl.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "if not gz_path.exists():\n",
    "    urlretrieve(MNIST_URL, gz_path)\n",
    "\n",
    "# File contains a tuple of tuples for the x and y, train and validation data\n",
    "# Images are 28x28\n",
    "with gzip.open(gz_path, \"rb\") as file:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(file, encoding=\"latin-1\")\n",
    "\n",
    "# Put into tensors\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c574d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26674fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets define some constants that describe the data\n",
    "n_examples, n_pixels = x_train.shape\n",
    "possible_values = y_train.max() + 1\n",
    "\n",
    "# How many nodes/activations/line thingys\n",
    "n_hidden = 50\n",
    "\n",
    "n_examples, n_pixels, possible_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "050c06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, n_hidden), nn.ReLU(), nn.Linear(n_hidden, n_out)]\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        res = inp\n",
    "        for layer in self.layers:\n",
    "            res = layer(res)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "506b3434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(n_pixels, n_hidden, 10)\n",
    "pred = model(x_train)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30474c",
   "metadata": {},
   "source": [
    "## Cross entropy loss\n",
    "\n",
    "We need to improve our loss function as MSE doesn't make sense, the distance between incorrecd predictions doesn't indicate how good/bad the prediction is. For example if the target is 2, then 3 isnt a better guess than 9.\n",
    "\n",
    "We are outputting a pred for each possible number. As only one is possible at a time our targets are a one hot encoded matrix. If our targets are going to sum to 1, then it makes sense that our preds do too. We can calculate the softmax for each pred. Then for our loss func we compare the softmaxes to the 1 hot encoded targets. As we are 1 hot encoded we can ignore the other targets and just take the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc463459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2991, -2.4952, -2.2559,  ..., -2.3145, -2.4055, -2.0550],\n",
       "        [-2.2196, -2.3798, -2.1516,  ..., -2.4936, -2.2933, -2.1178],\n",
       "        [-2.2478, -2.3300, -2.3284,  ..., -2.4150, -2.3338, -2.1713],\n",
       "        ...,\n",
       "        [-2.2851, -2.4577, -2.2501,  ..., -2.3418, -2.3174, -2.1544],\n",
       "        [-2.2231, -2.4281, -2.3472,  ..., -2.3487, -2.3482, -2.1276],\n",
       "        [-2.2634, -2.4369, -2.1781,  ..., -2.3236, -2.4562, -2.1039]],\n",
       "       grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_softmax(x):\n",
    "    return (x.exp() / (x.exp().sum(-1, keepdim=True))).log()\n",
    "\n",
    "\n",
    "log_softmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb4f0fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2991, -2.4952, -2.2559,  ..., -2.3145, -2.4055, -2.0550],\n",
       "        [-2.2196, -2.3798, -2.1516,  ..., -2.4936, -2.2933, -2.1178],\n",
       "        [-2.2478, -2.3300, -2.3284,  ..., -2.4150, -2.3338, -2.1713],\n",
       "        ...,\n",
       "        [-2.2851, -2.4577, -2.2501,  ..., -2.3418, -2.3174, -2.1544],\n",
       "        [-2.2231, -2.4281, -2.3472,  ..., -2.3487, -2.3482, -2.1276],\n",
       "        [-2.2634, -2.4369, -2.1781,  ..., -2.3236, -2.4562, -2.1039]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As log(a/b) = log(a) - log(b), we can simplify things to:\n",
    "\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1, keepdim=True).log()\n",
    "\n",
    "\n",
    "log_softmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc439ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0589, -0.1371,  0.1022,  ...,  0.0436, -0.0474,  0.3030],\n",
       "        [ 0.1145, -0.0457,  0.1824,  ..., -0.1596,  0.0408,  0.2162],\n",
       "        [ 0.0590, -0.0232, -0.0216,  ..., -0.1082, -0.0269,  0.1356],\n",
       "        ...,\n",
       "        [ 0.0561, -0.1165,  0.0911,  ..., -0.0005,  0.0239,  0.1868],\n",
       "        [ 0.1206, -0.0844, -0.0035,  ..., -0.0050, -0.0045,  0.2161],\n",
       "        [ 0.0676, -0.1058,  0.1530,  ...,  0.0074, -0.1251,  0.2272]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Its possible for the sum of the exponentials of big activations to overflow\n",
    "# pytorch uses some tricks to solve this for use in one function\n",
    "def log_softmax(x):\n",
    "    return x - x.logsumexp(-1, keepdim=True)\n",
    "\n",
    "\n",
    "sm_pred = log_softmax(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79b7fa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How do we index into the preds we want\n",
    "# The y values tell us targets, which are also the index\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fb4b202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.4828, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So for each i we want row i and col y_train[i]. eg for pred at row 0\n",
    "sm_pred[0, y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1e31cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.4828, -2.2196, -2.1388,  ..., -2.3174, -2.1172, -2.4562],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can do this for all of them like this\n",
    "sm_pred[range(y_train.shape[0]), y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "998fc2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3149, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So our cross entropy loss function looks like this\n",
    "def nll(inp, target):\n",
    "    return -inp[range(target.shape[0]), target].mean()\n",
    "\n",
    "\n",
    "loss = nll(sm_pred, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12045caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3149, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch gives us both of these functions, NLL = negative log likelihood\n",
    "F.nll_loss(F.log_softmax(pred, -1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d97107cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3149, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And a cross entropy function to do it all in one\n",
    "F.cross_entropy(pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a8668",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f3b7b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0589, -0.1371,  0.1022,  0.1158,  0.2218, -0.1247, -0.0787,  0.0436,\n",
       "         -0.0474,  0.3030], grad_fn=<SelectBackward0>),\n",
       " torch.Size([64, 10]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = F.cross_entropy\n",
    "batch_size = 64\n",
    "\n",
    "# First we would run a minibatch\n",
    "mini_batch = x_train[0:batch_size]\n",
    "mini_batch_y = y_train[0:batch_size]\n",
    "\n",
    "preds = model(mini_batch)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bf2c083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2940, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we would calculate the loss\n",
    "loss_func(preds, mini_batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fad34eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 9, 4, 4, 9, 4, 4, 9, 3, 9, 9, 9, 9, 9, 4, 2, 9, 4, 9, 4, 3, 2, 4, 4,\n",
       "        9, 9, 4, 9, 0, 3, 3, 2, 9, 4, 9, 4, 9, 9, 9, 9, 9, 9, 9, 4, 3, 3, 4, 9,\n",
       "        9, 9, 9, 3, 4, 4, 9, 9, 9, 9, 9, 9, 3, 4, 9, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each of our predictions what did we predict, we need the highest number from each set of preds and get its index\n",
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09e87d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False,  True, False, False, False,\n",
       "         True, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False, False,  True, False,\n",
       "        False, False, False,  True,  True, False, False,  True, False, False,\n",
       "        False,  True, False, False])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see which we got correct\n",
    "preds.argmax(dim=1) == mini_batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f0329b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1562)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use it to calculate accuracy, this isnt needed for the NN but helps us understand whats going on\n",
    "def accuracy(out, targets):\n",
    "    return (out.argmax(dim=1) == targets).float().mean()\n",
    "\n",
    "\n",
    "accuracy(preds, mini_batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b53cd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.6791051626205444 acc:  0.46875\n",
      "loss: 0.13027404248714447 acc:  0.953125\n",
      "loss: 0.10441920906305313 acc:  0.921875\n",
      "loss: 0.0580277256667614 acc:  0.96875\n",
      "loss: 0.07098586112260818 acc:  0.984375\n"
     ]
    }
   ],
   "source": [
    "# Now we want to do all of the batches for a bunch of epochs, updating the weights each time\n",
    "import torch\n",
    "\n",
    "lr = 0.5\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_start in range(0, n_examples, batch_size):\n",
    "        # Get the slice\n",
    "        sl = slice(batch_start, min(n_examples, batch_start + batch_size))\n",
    "\n",
    "        batch_inp = x_train[sl]\n",
    "        batch_targets = y_train[sl]\n",
    "\n",
    "        # Run the model\n",
    "        preds = model(batch_inp)\n",
    "        loss = loss_func(preds, batch_targets)\n",
    "\n",
    "        if batch_start == 0:\n",
    "            print(\"loss:\", loss.item(), \"acc: \", accuracy(preds, batch_targets).item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        with torch.no_grad():\n",
    "            for layer in model.layers:\n",
    "                if hasattr(layer, \"weight\"):\n",
    "                    layer.weight -= layer.weight.grad * lr\n",
    "                    layer.bias -= layer.bias.grad * lr\n",
    "                    layer.weight.grad.zero_()\n",
    "                    layer.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcadb23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
